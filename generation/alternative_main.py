# I initially created this model to use gemini (using an api key) and a public huggingface model (same in docs)
# But then cleaned up the code into functions to accomodate for customization
# Yet to test the code

import os
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI

DATA_PATH = 'Hackathon Planning 2024.pdf'

current_dir = os.path.dirname(os.path.abspath(__file__))

def create_vector_store(data_path, store_name, text_splitter, embedding_model):
    """
    Creates a vector store from a given data path and stores it in a persistent directory.

    Args:
        data_path (str): The path to the data file (e.g., a PDF file).
        store_name (str): The name of the vector store to be created.
        text_splitter (callable): A callable that splits the text into chunks.
        embedding_model (tuple): A tuple containing the embedding model and its configuration.

    Returns:
        Chroma: The created vector store.

    Raises:
        FileNotFoundError: If the file at the specified data path does not exist.

    Notes:
        If the vector store already exists in the persistent directory, it will be loaded instead of recreated.
    """
    file_path = os.path.join(current_dir, DATA_PATH)
    persistent_directory = os.path.join(current_dir, "db", store_name)
    if not os.path.exists(persistent_directory):
        print(f"\n--- Creating vector store ---")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The file {file_path} does not exist. Please check the path.")
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        text_splitter = text_splitter(chunk_size=1000, chunk_overlap=500)
        docs = text_splitter.split_documents(documents)
        embeddings =  embedding_model[0](model="embedding_model[1]")
        db = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)
        print(f"--- Finished creating vector store ---")
        return db
    else:
        print(f"Vector store already exists. No need to initialize.")
        embeddings =  embedding_model[0](model="embedding_model[1]")
        db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)
        return db


def query_vector_store(db, query, search_type, search_kwargs):
    """
    Queries the vector store to retrieve relevant documents based on the input query.

    Args:
        db (Chroma): The vector store to query.
        query (str): The input query to search for in the vector store.
        search_type (str): The type of search to perform (e.g., "dense", "exact").
        search_kwargs (dict): Additional keyword arguments for the search function.

    Returns:
        str: A combined input string that includes the original query and the relevant documents.

    Notes:
        This function uses the vector store's retriever to search for relevant documents based on the input query.
        The resulting documents are then combined with the original query to create a new input string.
    """
    retriever = db.as_retriever(
        search_type=search_type,
        search_kwargs=search_kwargs, 
    )
    relevant_docs = retriever.invoke(query)
    combined_input = (
        "Here are some documents that might help answer the question: "
        + query
        + "\n\nRelevant Documents:\n"
        + "\n\n".join([doc.page_content for doc in relevant_docs])
        + "\n\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'."
    )
    return combined_input

def generate_response (llm, combined_input):
    """
    Generates a response to the input query using a large language model (LLM).

    Args:
        llm (tuple): A tuple containing the LLM model and its configuration.
        combined_input (str): The input string that includes the original query and relevant documents.

    Returns:
        str: The generated response to the input query.

    Notes:
        This function uses the LLM to generate a response based on the input string.
        The response is generated by passing the input string to the LLM and retrieving the output.
    """
    llm = llm[0](model = llm[1])
    messages = [
        SystemMessage(content="You are a helpful assistant."),
        HumanMessage(content=combined_input),
    ]
    result = llm.invoke(messages)
    return result.content
